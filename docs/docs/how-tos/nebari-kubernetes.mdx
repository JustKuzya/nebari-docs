---
id: nebari-kubernetes
title: How to deploy Nebari on pre-existing infrastructure
description: Deploying Nebari on an existing Kubernetes infrastructure
---

If you have an existing kubernetes cluster running in the cloud and would like to deploy QHub on the same cluster, this is the guide for you.

To illustrate how this is done, the guide walks through a simple example. The guide below is meant to serve as a reference, the setup of your existing kubernetes might differ rending some of these additional setups steps unnecessary.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs className="unique-tabs">

  <TabItem value="AWS">

## AWS EKS cluster pre-requisites

In this example, there already exists a basic web app running on an EKS cluster.
[Here is the tutorial on how to setup this particular Guestbook web app](https://logz.io/blog/amazon-eks-cluster/).

The existing EKS cluster has one VPC with three subnets (each in their own Availability Zone) and no node groups. There are three nodes each running on a `t3.medium` EC2 instance,
unfortunately QHub's `general` node group requires a more powerful instance type.

Now create three new node groups in preparation for the incoming QHub deployment. Before proceeding, ensure the following:

- that the subnets can ["automatically assign public IP addresses to instances launched into it"](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html#subnet-public-ip).

- there exists an IAM role with the following permissions:

  - AmazonEKSWorkerNodePolicy

  - AmazonEC2ContainerRegistryReadOnly

  - AmazonEKS_CNI_Policy

  - The following custom policy:

  <details>

  ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "eksWorkerAutoscalingAll",
                "Effect": "Allow",
                "Action": [
                    "ec2:DescribeLaunchTemplateVersions",
                    "autoscaling:DescribeTags",
                    "autoscaling:DescribeLaunchConfigurations",
                    "autoscaling:DescribeAutoScalingInstances",
                    "autoscaling:DescribeAutoScalingGroups"
                ],
                "Resource": "*"
            },
            {
                "Sid": "eksWorkerAutoscalingOwn",
                "Effect": "Allow",
                "Action": [
                    "autoscaling:UpdateAutoScalingGroup",
                    "autoscaling:TerminateInstanceInAutoScalingGroup",
                    "autoscaling:SetDesiredCapacity"
                ],
                "Resource": "*",
                "Condition": {
                    "StringEquals": {
                        "autoscaling:ResourceTag/k8s.io/cluster-autoscaler/enabled": [
                            "true"
                        ],
                        "autoscaling:ResourceTag/kubernetes.io/cluster/eaeeks": [
                            "owned"
                        ]
                    }
                }
            }
        ]
    }
  ```

  </details>

## Create node groups

Skip this step if node groups already exist.

For AWS, [follow this guide to create new node groups](https://docs.aws.amazon.com/eks/latest/userguide/create-managed-node-group.html). Be sure to fill in the following fields
carefully:

- "Node Group configuration"
  - `Name` must be either `general`, `user` or `worker`
  - `Node IAM Role` must be the IAM role described proceeding
- "Node Group compute configuration"
  - `Instance type`
    - The recommended minimum vCPU and memory for a `general` node is 8 vCPU / 32 GB RAM
    - The recommended minimum vCPU and memory for a `user` and `worker` node is 4 vCPU / 16 GB RAM
  - `Disk size`
    - The recommended minimum is 200 GB for the attached EBS (block-strage)
- "Node Group scaling configuration"
  - `Minimum size` and `Maximum size` of 1 for the `general` node group
- "Node Group subnet configuration"
  - `subnet` include all existing EKS subnets

## Deploy QHub to Existing EKS Cluster

Ensure that you are using the existing cluster's `kubectl` context.

Initialize in the usual manner:

```bash
  qhub init aws \
    --project <project_name> \
    --domain <domain_name> \
    --ci-provider github-actions \
    --auth-provider github \
    --auth-auto-provision
```

Then update the `qhub-config.yaml` file. The important keys to update are:

- Replace `provider: aws` with `provider: local`
- Replace `amazon_web_services` with `local`
  - And update the `node_selector` and `kube_context` appropriately

<details>
  <summary>Updated qhub-config.yaml section</summary>

```yaml
existing:
  kube_context: arn:aws:eks:<region>:xxxxxxxxxxxx:cluster/<existing_cluster_name>
  node_selectors:
    general:
      key: eks.amazonaws.com/nodegroup
      value: general
    user:
      key: eks.amazonaws.com/nodegroup
      value: user
    worker:
      key: eks.amazonaws.com/nodegroup
      value: worker
```

</details>

Once updated, deploy QHub. When prompted be ready to manually update the DNS record.

- `local` or "existing" deployments fail if you pass `--dns-auto-provision` or `--disable-prompt`

```
python -m qhub deploy --config qhub-config.yaml
```

The deployment completes successfully and all the pods appear to be running and so do the pre-existing Guestbook web app.

</TabItem>

<TabItem value="Azure">
</TabItem>

<TabItem value="DO">
</TabItem>

<TabItem value="GCP">
</TabItem>

</Tabs>
